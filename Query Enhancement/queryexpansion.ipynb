{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499d54e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Win 11\\Desktop\\RAG Pipeline\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e6e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step1 : Load and split the dataset\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19f7c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000017D16EAF920>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### step 2: Vector Store\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(chunks,embedding_model)\n",
    "\n",
    "## step 3:MMR Retriever\n",
    "retriever=vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":5})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77414d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000017D175206B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017D1777FFB0>, root_client=<openai.OpenAI object at 0x0000017D16EAFAD0>, root_async_client=<openai.AsyncOpenAI object at 0x0000017D175777D0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 4 : LLM and Prompt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(\"openai:o4-mini\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f698364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\n    You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\\n    Original query: \"{query}\"\\n    Expanded query:\\n    ')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000017D175206B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017D1777FFB0>, root_client=<openai.OpenAI object at 0x0000017D16EAFAD0>, root_async_client=<openai.AsyncOpenAI object at 0x0000017D175777D0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\n",
    "    Original query: \"{query}\"\n",
    "    Expanded query:\n",
    "    \"\"\")\n",
    "\n",
    "query_expansion_chain = query_expansion_prompt | llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e937d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expanded query:  \\n“LangChain memory” OR “LangChain memory management” OR “LangChain conversational memory” OR “LangChain session memory” OR “LangChain context persistence” OR “LangChain state management” OR “LangChain long-term memory” OR “LangChain short-term memory” OR “LangChain buffer memory” OR “ConversationBufferMemory” OR “ChatMessageHistory” OR “MemoryChain” OR “RetrievalAugmentedMemory” OR “vector store memory” OR “embedding-based memory” OR “AI agent memory” OR “prompt context storage” OR “multi-turn dialogue memory” OR “contextual memory management in LangChain (Python)” OR “LangChain memory integration with Redis/Pinecone/Chroma” OR “LangChain memory optimization”'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\":\"Langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d75d6596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG answering prompt\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Answer the question based on the context below.\n",
    "\n",
    "    Context:\n",
    "    \"{context}\n",
    "\n",
    "    Question: {input}\n",
    "    \"\"\")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706fe89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline with query expansion\n",
    "rag_pipeline = (\n",
    "       RunnableMap({\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]}))\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fc36d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline with query expansion\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]}))\n",
    "    })\n",
    "       | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e40b9b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:  \n",
      "“LangChain memory support types” OR “LangChain memory modules” OR “LangChain memory API”  \n",
      "AND (conversation memory OR chat‐history persistence OR context management OR session state OR memory augmentation)  \n",
      "AND (BufferMemory OR ConversationBufferMemory OR ConversationSummaryMemory OR TokenBufferMemory OR ChatMessageHistory)  \n",
      "AND (in-memory OR RedisMemory OR SQLMemory OR file-based OR browser-storage memory backends)  \n",
      "AND (vector store integration OR Pinecone OR FAISS OR Chroma OR Weaviate)  \n",
      "AND (memory retriever OR memory management OR state persistence OR context summarization)  \n",
      "AND (LangChain Python SDK OR LangChain JavaScript SDK)\n",
      "✅ Answer:\\n LangChain supports at least two built-in memory modules out of the box:  \n",
      "- ConversationBufferMemory – keeps the full recent exchange history in memory  \n",
      "- ConversationSummaryMemory – compresses long chats into a concise summary to stay within token limits\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c12549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:\n",
      "\n",
      "“CrewAI agents” OR “Crew AI autonomous agents” OR “CrewAI multi-agent system” OR “CrewAI platform” OR “AI agent orchestration” OR “CrewAI API” OR “CrewAI SDK” OR “AI-powered digital assistants” OR “autonomous workflow automation” OR “enterprise AI agent framework” OR “digital workforce” OR “AI-driven virtual collaborators” OR “CrewAI use cases” OR “CrewAI technical documentation”\n",
      "✅ Answer:\\n CrewAI agents are autonomous, semi-independent “workers” in a multi-agent system, each defined by:  \n",
      "• A clear purpose and goal  \n",
      "• A designated role (e.g. researcher, planner, executor)  \n",
      "• A set of tools and memory they may use  \n",
      "\n",
      "Key characteristics:  \n",
      "1. Structured Collaboration – Agents form a workflow, handing off tasks and results according to their roles.  \n",
      "2. Configurable via YAML/JSON – Developers specify each agent’s role, goals, memory, and tools in a simple declarative config.  \n",
      "3. Autonomous Orchestration – CrewAI manages the agent loop, turn-taking, tool invocation, and decision-making so that each agent stays on task.  \n",
      "4. Horizontal & Vertical Scaling – You can add more agents (horizontal) or stack deeper reasoning chains (vertical) to tackle increasingly complex objectives.  \n",
      "\n",
      "In short, a CrewAI agent is a purpose-driven AI sub-module that collaborates within a crew to collectively achieve larger, structured goals.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"CrewAI agents?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
