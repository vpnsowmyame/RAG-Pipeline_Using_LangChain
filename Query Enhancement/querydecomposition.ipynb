{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2227e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Win 11\\Desktop\\RAG Pipeline\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba05152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5963e511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000261917D0FE0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000026190F29310>, root_client=<openai.OpenAI object at 0x000002618EADCE60>, root_async_client=<openai.AsyncOpenAI object at 0x00000261917154C0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"openai:o4-mini\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9acf2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "    Question: \\\"{question}\\\"\n",
    "\n",
    "    Sub-questions:\n",
    "    \"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f37a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-questions:\n",
      "\n",
      "1. What memory architectures and storage mechanisms does LangChain employ in its workflows?  \n",
      "2. How does LangChain define, orchestrate and execute its agents?  \n",
      "3. What memory approaches and data-persistence features does CrewAI provide?  \n",
      "4. How are agents designed, managed and invoked within CrewAI?\n"
     ]
    }
   ],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n",
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61173e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Use the context below to answer the question.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {input}\n",
    "    \"\"\")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5659498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-•1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "\n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "\n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cb7eb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Answer:\\n\n",
      "Q: Here are four focused sub-questions you can use to guide your document search:\n",
      "A: 1. Which external tools and environments does LangChain support connecting to (e.g., web search, calculators, code execution environments, custom APIs)?  \n",
      "2. What vector databases does LangChain integrate with out-of-the-box (e.g., FAISS, Chroma, Pinecone, Weaviate)?  \n",
      "3. How does LangChain enable semantic search over large document collections?  \n",
      "4. In the context of Retrieval-Augmented Generation (RAG), how is external knowledge fetched from these sources and injected into the language model?\n",
      "\n",
      "Q: What memory abstractions and storage options does LangChain provide, and how are they configured?\n",
      "A: LangChain’s “memory” subsystem is deliberately factored into two orthogonal pieces:  \n",
      "  1. A memory abstraction (the interface – what gets stored and how you read it back)  \n",
      "  2. One or more concrete storage backends  \n",
      "\n",
      "You pick a memory class (or compose several) to define *what* you want to remember, and you pick/configure a backend to define *where* it lives and whether it persists across restarts.\n",
      "\n",
      "1. Memory abstractions  \n",
      "   • ConversationBufferMemory  \n",
      "     – Simply accumulates every user ↔ assistant turn in a list or message‐history object.  \n",
      "     – Constructor args:  \n",
      "       ◦ memory_key (default “history”)  \n",
      "       ◦ input_key / output_key (which fields of your I/O dict to capture)  \n",
      "       ◦ return_messages=True/False (raw Message objects vs. formatted strings)  \n",
      "   • ConversationBufferWindowMemory  \n",
      "     – Like BufferMemory but keeps only the last k turns.  \n",
      "     – Extra arg: k (window size)  \n",
      "   • ConversationSummaryMemory  \n",
      "     – Keeps a running summary rather than the full transcript, so you never blow past the LLM’s context window.  \n",
      "     – Requires an LLM (to do the summarization) and lets you supply or customize the summarization chain.  \n",
      "     – Params include memory_key, llm, max_token_limit, etc.  \n",
      "   • CombinedMemory  \n",
      "     – Wraps multiple memory modules at once (e.g. a buffer plus a summary).  \n",
      "   • (Plus more specialized modules)  \n",
      "     – e.g. ConversationKGMemory (store relations in a small knowledge graph), or custom VectorStore–based memories.  \n",
      "\n",
      "   All of these implement the same interface (load_memory_variables, save_context) so you can drop them into any Chain or Agent.\n",
      "\n",
      "2. Storage backends  \n",
      "   By default most memory classes are purely in‐memory (i.e. lost when your process shuts down).  To get persistence, LangChain provides:  \n",
      "   • RedisMemory (backed by Redis)  \n",
      "   • SQLAlchemyMemory (backed by any SQL database)  \n",
      "   • MongoMemory (backed by MongoDB)  \n",
      "   • VectorStore–based memory (Chroma, Pinecone, Weaviate, etc.)  \n",
      "   • File-based JSON or pickle (via simple wrappers)  \n",
      "   • …and you can write your own by subclassing BaseMemory and plugging in any key/value store.  \n",
      "\n",
      "   Typical configuration parameters:  \n",
      "     – connection_string (for Redis/Mongo/SQL)  \n",
      "     – table_name or collection_name  \n",
      "     – persist_directory (for file or vector stores)  \n",
      "     – embedding function + index_name (for vector stores)  \n",
      "\n",
      "3. Bringing it together  \n",
      "   In practice you do something like:  \n",
      "   ```python\n",
      "   from langchain.memory import ConversationSummaryMemory\n",
      "   from langchain import OpenAI, LLMChain\n",
      "\n",
      "   memory = ConversationSummaryMemory(\n",
      "       llm=OpenAI(temperature=0),\n",
      "       memory_key=\"chat_history\",\n",
      "       max_token_limit=512\n",
      "   )\n",
      "   chain = LLMChain(\n",
      "       llm=OpenAI(),\n",
      "       prompt=my_prompt_template,\n",
      "       memory=memory\n",
      "   )\n",
      "   ```\n",
      "   Or, for a persisted Redis buffer window:  \n",
      "   ```python\n",
      "   from langchain.memory import ConversationBufferWindowMemory, RedisChatMessageHistory\n",
      "\n",
      "   history = RedisChatMessageHistory(redis_url=\"redis://localhost:6379/0\", session_id=\"user123\")\n",
      "   memory = ConversationBufferWindowMemory(\n",
      "       chat_memory=history,\n",
      "       k=5,\n",
      "       return_messages=True\n",
      "   )\n",
      "   ```\n",
      "   Once configured, LangChain will automatically call `memory.save_context(...)` after each turn and will inject `memory.load_memory_variables(inputs)` into your prompt template under the key you specified (e.g. `chat_history`).\n",
      "\n",
      "—  \n",
      "In this way, you mix-and-match *what* you remember (full buffer, window, summary, graph, vectors…) with *where* you store it (in RAM, Redis, SQL, vectors…) simply by swapping or composing memory classes and passing them into your Chains or Agents.\n",
      "\n",
      "Q: How does LangChain’s agent framework work (e.g. tool invocation, planning and decision loops)?\n",
      "A: At a high level, a LangChain “agent” is nothing more exotic than an LLM-backed dispatcher that (1) figures out what steps need to happen, (2) picks and calls the right tool for each step, (3) ingests the tool’s output back into its working memory, and (4) repeats until it’s done.  Under the hood it really boils down to two cooperating loops—a planner and an executor:\n",
      "\n",
      "1. Tool Registry  \n",
      "   • Before you even ask your agent a question, you give it a catalog of “tools.”  \n",
      "   • A tool is just a little wrapper around some functionality—e.g. a Python REPL, a web-search API, a calculator, a database query, whatever you need.  \n",
      "   • Each tool has a name, a description (for the LLM), and a function signature.\n",
      "\n",
      "2. The Planner (“What do I do?”)  \n",
      "   • When you send the agent a user query, the first thing it does is invoke a planning chain (often an LLM prompt with system instructions like “Break this goal into substeps.”)  \n",
      "   • The planner LLM emits a rough outline or sequence of steps, each step usually naming a tool plus any free-form reasoning or notes.  \n",
      "   • In more advanced setups you can even get branching plans (“If search finds X, then do A; otherwise do B.”)  \n",
      "\n",
      "3. The Executor (“Let’s do it, step by step.”)  \n",
      "   • The executor loop walks through each planned step in order.  \n",
      "   • For each step:  \n",
      "     – It re-prompts the LLM with the current step description plus a “scratchpad” of everything done so far.  \n",
      "     – The LLM outputs a JSON or structured snippet telling which tool to call and with what arguments.  \n",
      "     – LangChain invokes exactly that tool function, captures the output, and appends it to the scratchpad/memory.  \n",
      "   • This forms the classic ReAct–style “thought → action → observation → thought” loop.\n",
      "\n",
      "4. Dynamic Decision Making & Branching  \n",
      "   • You’re not locked into your original plan. After any observation the executor can re-invoke the planner (or even the same reasoning chain) to decide “should I pivot to a different subgoal?”  \n",
      "   • You can embed conditional logic so that if one tool “fails” or returns unexpected results, you automatically switch to a backup tool or a new line of inquiry.\n",
      "\n",
      "5. Memory & Context Across Steps  \n",
      "   • Everything that happened—your user’s question, every thought the LLM had, every tool call and its result—can be stored in a memory buffer or external store.  \n",
      "   • That memory can be retrieved and summarized when you loop back to the planner or are constructing your final answer, giving true context‐awareness.\n",
      "\n",
      "6. Termination & Final Answer  \n",
      "   • Eventually the LLM signals a “final answer” (often via a special stop token or by hitting a self-declared end step).  \n",
      "   • The agent then returns that answer to the user, along with (optionally) a transcript of its chain-of-thought and tool‐invocation history.\n",
      "\n",
      "Putting it all together, you end up with a system that:  \n",
      " – Uses an LLM to think (planner)  \n",
      " – Uses an LLM to choose and parameterize actions (executor)  \n",
      " – Calls real code/APIs (tools)  \n",
      " – Feeds results back into its context or memory  \n",
      " – Adapts mid‐stream if things change or go wrong  \n",
      " – Keeps going until the original user goal is satisfied\n",
      "\n",
      "That “planner → executor → tool → observation → (optionally re-plan) → … → final answer” cycle is at the core of every LangChain agent.\n",
      "\n",
      "Q: What memory management mechanisms and backends does CrewAI offer?\n",
      "A: CrewAI’s goal is to remain completely agnostic about how you store “memory” or which LLMs you hook up to it, so both are fully pluggable.  \n",
      "\n",
      "1.  Memory‐management mechanisms  \n",
      "    •  Ephemeral (in-RAM) session memory – for simple, short-lived context windows  \n",
      "    •  Persistent “long-term” memory – chunked, embedded and stored in a vector store for retrieval.  \n",
      "    •  Built-in support for LRU or threshold-based trimming of old memories  \n",
      "    •  A simple, in-process dict/list implementation ships out of the box, but you can swap in any of:  \n",
      "       –  Redis (via RedisVectorStore)  \n",
      "       –  FAISS, Chroma, Qdrant, Milvus, Weaviate, Pinecone, etc.  \n",
      "       –  Plain SQL or MongoDB (via community connectors)  \n",
      "    •  Hooks for custom memory stores: implement a 2–3 method interface and drop it in  \n",
      "\n",
      "2.  LLM backends  \n",
      "    •  OpenAI (GPT-3.5, GPT-4, GPT-4‐Turbo)  \n",
      "    •  Azure OpenAI  \n",
      "    •  Anthropic Claude  \n",
      "    •  Cohere  \n",
      "    •  Amazon Bedrock  \n",
      "    •  Hugging Face Inference (hosted or local)  \n",
      "    •  Local LLMs via Llama.cpp / Ollama / any HF-Compatible model  \n",
      "    •  Plus any other provider that LangChain (or your own adapter) can wrap  \n",
      "\n",
      "Because both memories and LLMs are exposed as small, well‐defined plugins, you can switch from in-memory to Redis or from GPT-4 to a local Llama model with a one-line config change.\n",
      "\n",
      "Q: How does CrewAI implement and orchestrate agents for LLM-based workflows?\n",
      "A: CrewAI’s core idea is to let you think of your LLM-powered workflow not as one giant agent but as a small “crew” of specialized agents that collaborate. Here’s roughly how it works under the hood:\n",
      "\n",
      "1. Agent and Crew abstractions  \n",
      "   • You define one or more Agent classes (e.g. PlannerAgent, ResearchAgent, WriterAgent), each of which encapsulates:  \n",
      "     – A role or “skill set” (prompt template, tool set, memory schema)  \n",
      "     – A step() or act() method that accepts incoming messages/context and emits actions or replies  \n",
      "   • You group those Agents into a Crew (an ordered or named collection).\n",
      "\n",
      "2. Shared context and memory  \n",
      "   • There is a Crew-level context store (or message bus) where Agents can read/write:  \n",
      "     – Task description or current objective  \n",
      "     – Intermediate results, findings, documents  \n",
      "     – Conversation history and tool outputs  \n",
      "   • Optionally each Agent can also have its own local memory or tool cache.\n",
      "\n",
      "3. Orchestration engine (the “Crew runner”)  \n",
      "   • When you submit a top-level task, the runner:  \n",
      "     1. Loads or instantiates each Agent in the crew  \n",
      "     2. Feeds the initial task prompt into a designated “lead” or “planner” Agent  \n",
      "     3. Enters an event loop:  \n",
      "        – Picks the next Agent(s) to run (round-robin, priority queue, or rule based)  \n",
      "        – Delivers the latest shared context/messages to its step() method  \n",
      "        – Gathers its output (a piece of text, a function/tool call, or a subtask assignment)  \n",
      "        – Updates the shared context and may trigger new tool invocations or spawn helper Agents  \n",
      "   • Loop continues until a stopping condition (e.g. final answer produced, max iterations reached).\n",
      "\n",
      "4. Dynamic delegation and pipelining  \n",
      "   • Agents can delegate subtasks to other Agents by posting messages tagged with the recipient’s name.  \n",
      "   • You can build pipelines: e.g. ResearchAgent → SummarizerAgent → CriticAgent → WriterAgent  \n",
      "   • Agents can also call external tools (databases, search APIs, calculators) and post results back to the crew.\n",
      "\n",
      "5. Extensibility & customization  \n",
      "   • You register new Agent types or tools simply by subclassing the base Agent class and providing a prompt and action schema.  \n",
      "   • You configure the orchestration policy (how agents are scheduled, how messages are routed) via a small YAML or Python config.  \n",
      "   • Behind the scenes all LLM calls go through a pluggable LLM interface (OpenAI, Azure OpenAI, local Llama-style backends).\n",
      "\n",
      "In practice this means you never have to write one massive prompt or monolithic LLM loop. Instead you spin up a small team of tight-scope agents, let them pass context among themselves, and let the Crew runner orchestrate who talks, who listens, who calls which tool, and when the job is done.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"✅ Final Answer:\\\\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
